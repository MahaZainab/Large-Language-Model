# Large Language Models

## LLM Quantization:
Quantization is a compression technique that involves mapping high precision values to the lower precision one. For an LLM, that means modifying the presion of weights and activations making it less memory intensive. But remember this is surely going to have an impact on the 